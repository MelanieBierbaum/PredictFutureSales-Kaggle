{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n\n# load packages\nimport sys # access to system parameters https://docs.python.org/3/library/sys.html\nprint(\"Python version: {}\". format(sys.version))\n\nimport numpy as np # scientific computing\nprint(\"NumPy version: {}\". format(np.__version__))\n\nimport pandas as pd # data processing and analysis \nprint(\"pandas version: {}\". format(pd.__version__))\n\nimport matplotlib\nimport matplotlib.pyplot as plt # plotting\n%matplotlib inline \nprint(\"matplotlib version: {}\". format(matplotlib.__version__))\n \nimport sklearn # machine learning algorithms\nprint(\"scikit-learn version: {}\". format(sklearn.__version__))\n\nfrom itertools import product\nfrom xgboost import XGBRegressor\nfrom xgboost import plot_importance\nimport time\nfrom sklearn.preprocessing import LabelEncoder\n\nprint(\"----------\")\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load Data\nEither original data files or the ones prepared in PFS-dataAnalysis&cleanup.\n"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# read original data\ndf_items = pd.read_csv('../input/competitive-data-science-predict-future-sales/items.csv')\n#df_shops = pd.read_csv('../input/competitive-data-science-predict-future-sales/shops.csv') # read cleaned version instead later\ndf_sales = pd.read_csv('../input/competitive-data-science-predict-future-sales/sales_train.csv')\ndf_test = pd.read_csv('../input/competitive-data-science-predict-future-sales/test.csv')\n#df_item_cat = pd.read_csv('../input/competitive-data-science-predict-future-sales/item_categories.csv')\n\n# read my versions\ndf_shops = pd.read_csv('../input/pfs-dataanalysis-cleanup/df_shops_cleaned.csv')\ndf_shops.drop(df_shops.columns[0], axis= 1, inplace=True)\nmonth_info = pd.read_pickle('../input/pfs-dataanalysis-cleanup/month_info.pkl')\ndf_item_cats = pd.read_csv('../input/pfs-dataanalysis-cleanup/df_item_cat_cleaned.csv')\ndf_item_cats.drop(df_item_cats.columns[0], axis= 1, inplace=True)\nall_data = pd.read_pickle('../input/pfs-dataanalysis-cleanup/all_data_cleaned.pkl')\nall_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Mean Encodings\nLike done in Programming Assignment week 3. The mean encodings are only calculated on the training part of the data. So let's do this before concatenating train and test. \n\nUnfortunatelly they still generate target leakage. None of them is used in the final model. Any suggestions about what is wrong are very welcome."},{"metadata":{"trusted":true},"cell_type":"code","source":"# use code from PA w3\nts = time.time()\n# simple item target encoding - ATTENTION do not use as feature due to leakage/overfitting\nitem_id_target_mean = all_data.groupby('item_id').target.mean()\nall_data['item_target_enc'] = all_data['item_id'].map(item_id_target_mean)\nglobal_mean = all_data.item_target_enc.mean()\nall_data['item_target_enc'].fillna(global_mean, inplace=True) # for the new items in date_block_num 34 no mean can be calculated, fill with global_mean\n\n# k fold encoding\nfrom sklearn.model_selection import KFold\nkf = KFold(5, shuffle=False)\nall_data['item_target_enc_kf'] = np.nan\n\nfor train_index, val_index in kf.split(all_data):\n    X_train, X_val = all_data.iloc[train_index, :], all_data.iloc[val_index, :]\n    fold_means = X_train.groupby(\"item_id\").target.mean()\n    means = X_val[\"item_id\"].map(fold_means).values\n    all_data.iloc[val_index, np.where(all_data.columns=='item_target_enc_kf')[0]] = means\n\nall_data['item_target_enc_kf'].fillna(global_mean, inplace=True) # item_ids not present in a fold will get nan, they are filled with global_mean\n\n# leave one out encoding\nsumme = all_data.groupby('item_id')['target'].transform('sum')\ntgo = all_data.target\nn_objects = all_data.groupby('item_id')['target'].transform('count')\nall_data['item_target_enc_loo'] = (summe - tgo) / (n_objects -1)\nall_data['item_target_enc_loo'].fillna(global_mean, inplace=True) # new items in 34 got 0, set to global_mean\n\n# smoothing\nnrows = all_data.groupby('item_id')['target'].transform('count')\nmeann = all_data.groupby('item_id').target.transform('mean')\nall_data['item_target_enc_sm'] = (meann*nrows + (global_mean*100))/(nrows+100)\n\n# Expanding Mean Scheme\ncumcnt = all_data.groupby('item_id').target.cumcount()\ncumsumm = all_data.groupby('item_id').target.cumsum()\nall_data['item_target_enc_ems']= (cumsumm -all_data.target)/cumcnt\nall_data.item_target_enc_ems = all_data.item_target_enc_ems.fillna(global_mean)\n\n# downcast all columns that are needed later\nall_data.item_target_enc_kf = all_data.item_target_enc_kf.astype(np.float32)\nall_data.item_target_enc_loo = all_data.item_target_enc_loo.astype(np.float32)\nall_data.item_target_enc_sm = all_data.item_target_enc_sm.astype(np.float32)\nall_data.item_target_enc_ems = all_data.item_target_enc_ems.astype(np.float32)\n# drop no longer needed column\nall_data = all_data.drop(columns='item_target_enc')\n\n# get means of the mean encodings \ncol = ['item_target_enc_kf','item_target_enc_loo','item_target_enc_sm','item_target_enc_ems']\ngroup = all_data.groupby('item_id')[col].mean()\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# prepare df_test with the same columns as all_data\ndf_test[\"date_block_num\"] = 34\ndf_test[\"target\"] = np.nan\ndf_test = df_test.set_index(\"ID\")\ndf_test = pd.merge(df_test, group, on='item_id', how='left')\ndf_test[col] = df_test[col].fillna(global_mean) # fill means of mean encodings\ndf_test.head(7)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Merge train and test data. Makes feature engineering more simple."},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data = all_data.append(df_test, ignore_index = False) # changed to False now to keep track of IDs in the test set. index restarts for month 34. But this is overwritten later anyway...\nall_data.head(7)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Add category ID from df_item_cat and info about monthly data from month_info."},{"metadata":{"trusted":true},"cell_type":"code","source":"# add category id to trainset and testset (merge 'left', otherwise this sorts the data by item id, then by (kept) data_block_num, shop_id)\nall_data = pd.merge(all_data, df_items[['item_id', 'item_category_id']], on = 'item_id', how='left')\nall_data = pd.merge(all_data, month_info, on = 'date_block_num', how='left')\n# set values for month 34 (test set)\nall_data.loc[all_data.date_block_num == 34 ,'days_per_month'] = 30\nall_data.loc[all_data.date_block_num == 34 ,'month'] = 11\n\n# downcast for better memory usage\nall_data['shop_id'] = all_data['shop_id'].astype(np.int8)\nall_data['item_id'] = all_data['item_id'].astype(np.int16)\nall_data['item_category_id'] = all_data['item_category_id'].astype(np.int8)\nall_data['days_per_month'] = all_data['days_per_month'].astype(np.int8)\nall_data['month'] = all_data['month'].astype(np.int8)\n\nall_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Label Encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"# use LabelEncoder to generate city_code\nle = LabelEncoder()\ndf_shops['city_code']= le.fit_transform(df_shops.city)\ndf_item_cats['item_bcat_code'] = le.fit_transform(df_item_cats.item_broad_cat)\ndf_item_cats['item_scat_code'] = le.fit_transform(df_item_cats.item_sub_cat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# merge additional shop columns into all_data\nall_data = pd.merge(all_data, df_shops[['shop_id','city_code']], on='shop_id', how='left' )\nall_data = pd.merge(all_data, df_item_cats[['item_category_id','item_bcat_code', 'item_scat_code']], on='item_category_id', how='left')\n\n# downcast for better memory usage\nall_data['city_code'] = all_data['city_code'].astype(np.int8)\nall_data['item_bcat_code'] = all_data['item_bcat_code'].astype(np.int8)\nall_data['item_scat_code'] = all_data['item_scat_code'].astype(np.int8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# free up memory\ndel df_shops\ndel df_item_cats\ndel month_info","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Lag Features\n\nProject Advice week 3:\n\nYou can get a rather good score after creating some lag-based features like in advice from previous week and feeding them into gradient boosted trees model.\n\nApart from item/shop pair lags you can try adding lagged values of total shop or total item sales (which are essentially mean-encodings). All of that is going to add some new information.\n\nLet's do that."},{"metadata":{"trusted":true},"cell_type":"code","source":"# lag function, designed to work with all_data and different lags for just one column at the moment\n\ndef lag_feature (df, lag, column):\n    # create a temporary df with just the columns needed to merge back and calculate the lag\n    temp = df[['shop_id', 'item_id', 'date_block_num', column]]\n    # loop to go through each lag time (number of month)\n    for i in lag:\n        shifted = temp.copy()\n        # give the columns of the shifted df the \"lag name\"\n        shifted.columns = ['shop_id','item_id','date_block_num',column+'_lag_'+str(i)]\n        # increase month number\n        shifted['date_block_num'] += i\n        # merge it back to the original df given to the function. 'left' -> keep original keys\n        df = pd.merge(df, shifted, on=['shop_id','item_id','date_block_num'], how='left')\n    return df\n\n# shop_id, item_id, date_block_num combinations that are not present in the month before are filled with Nan\n# this means that in the first month(s) ALL lag values are Nan","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Is it better to have the lag features for sum or mean? I used sum because the final project advice said \"total shop or item sales\". I tried mean as well. The validation rsme 1 and the public score were a bit higher (i.e worse). See Version 13 compared to Version 14. Changed it back to sum."},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\n\n# create new columns for total item sales (over all shops) and total shop sales (over all items) in each month\n# month_item_sum -> how many times was an item_id sold over all shops in each month?\n# month_shop_sum -> how many items were sold in each shop in each month?\n\ngroup = all_data.groupby(['date_block_num', 'item_id']).agg({'target': 'sum'})\ngroup.columns = ['month_item_sum']\ngroup.reset_index(inplace=True)\nall_data = pd.merge(all_data, group, on=['date_block_num','item_id'], how='left')\n\ngroup = all_data.groupby(['date_block_num', 'shop_id']).agg({'target': 'sum'})\ngroup.columns = ['month_shop_sum']\ngroup.reset_index(inplace=True)\n\nall_data = pd.merge(all_data, group, on=['date_block_num','shop_id'], how='left')\n\n\n# add lag features for all the month given in lag_time\nlag_time = [1,2,3,4,5, 12]\n\nall_data = lag_feature(all_data, lag_time, 'target')  # shop-item lag\nall_data = lag_feature(all_data, lag_time, 'month_item_sum') # total item lag\nall_data = lag_feature(all_data, lag_time, 'month_shop_sum') # total shop lag\nall_data = lag_feature(all_data, lag_time, 'month_sum') # total month lag, no of items sold per month\nall_data = lag_feature(all_data, lag_time, 'monthly_transactions') # total month lag, no of transactions per month\n\n\n# fill Nan in the lag columns with 0 as no value means the item has not been sold the month before\ncolumnliste = list(all_data.columns)\nlaglist=[]\n# create a list with all columns containing 'lag'\nfor element in columnliste:\n    if 'lag' in element:\n        laglist.append(element)\n        \n# fill NaNs for all lag columns and downcast       \nfor element in laglist:\n    all_data[element].fillna(0, inplace=True)\n    all_data[element] = all_data[element].astype(np.float32)\n\n# drop the columns that were used to generate lag features, they are no longer needed\nall_data.drop(columns= ['month_item_sum','month_shop_sum', 'monthly_transactions', 'month_sum'], axis= 1, inplace=True)\nall_data.head(5)\n    \ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Price Features\nExperiment with adding (average) item price. Pure average price gave worse results. Use price trend instead."},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove outlies from df_sales, like it has been done when generating all_data\ndf_sales = df_sales[df_sales.item_price < 100000]\ndf_sales = df_sales[df_sales.item_cnt_day < 1000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\n\n# get average price per item_id over whole time period\ngroup = df_sales.groupby('item_id').agg({'item_price' : 'mean'})\ngroup.columns = ['avg_price']\nall_data = pd.merge(all_data, group, on= 'item_id', how = 'left')\nall_data['avg_price'] = all_data['avg_price'].astype(np.float32)\n\n# get average price per item_id in each month\ngroup = df_sales.groupby( [\"date_block_num\",\"item_id\"] ).agg( {\"item_price\": [\"mean\"]} )\ngroup.columns = [\"month_avg_price\"]\nall_data = all_data.merge(group, on = [\"date_block_num\",\"item_id\"], how = \"left\")\nall_data[\"month_avg_price\"] = all_data['month_avg_price'].astype(np.float32)\n\n# create lags for average item price in a month, i.e. how much did this item cost in average the month before? the two month before?...\n# the lag columns contain NaN when an item has not been sold the 1/2/3 month before\nlag_time = [1, 2, 3]\nall_data = lag_feature(all_data, lag_time, 'month_avg_price' )\n\n# calculate by how much percent the item price for the last 1/2/3 month was above/below overall average price\n# the lags have NaNs where the price change could not be computed\nfor i in lag_time:\n    all_data[\"delta_price_lag_\" + str(i) ] = (all_data[\"month_avg_price_lag_\" + str(i)]- all_data[\"avg_price\"]) / all_data[\"avg_price\"]\n\n# calculate if there was a price drop/rise in the last month\n# the lags have NaNs where the price change could not be computed\nall_data[\"price_trend\"] = (all_data[\"month_avg_price_lag_1\"] - all_data[\"month_avg_price_lag_2\"]) / all_data[\"month_avg_price_lag_1\"]\n# (what has the item cost the month before - what does it cost this month) / what does it cost this month --- cannot be comupted due to the NaNs in month 34, instad:\n# (what has the item cost the month before - what has it cost 2 month before) / what has the item cost the month before\n\nts = time.time()\ndef select_trends(row) :\n    for i in lag_time:\n        if row[\"delta_price_lag_\" + str(i)]:\n            return row[\"delta_price_lag_\" + str(i)]\n    return 0\n\n### \nall_data[\"delta_price_lag_1\"].fillna( 0 ,inplace = True)\nall_data[\"delta_price_lag_2\"].fillna( 0 ,inplace = True)\nall_data[\"delta_price_lag_3\"].fillna( 0 ,inplace = True)\nall_data[\"price_trend\"].fillna( 0 ,inplace = True)\n###\n\nall_data[\"acc_price_lag\"] = all_data.apply(select_trends, axis = 1)\nall_data[\"acc_price_lag\"] = all_data.acc_price_lag.astype(np.float32 )\ntime.time() - ts\n\n\n# the function select_trends goes through every delta_price_lag_1/2/3 column and checks if there is a value present\n# if yes then this value is put into the column delta price_lag, if no then the next delta_price_lag_2/3 is checked\n# if nothing is found then 0 is returned\n# this function is applied to every row of the matrix. That is why it takes so long. \n# delta_price_lag contains the percentual change in price compared to the overall average price (for the last 1/2/3 month)). \n# i.e. in the last month, how much higher/lower was the price compared to the overall average price?\n# Note: there will be 0 for the new items in month 34, as they dont have a price, no price related features can be calculated.\n\n# drop the no longer needed columns\ncolumnliste = list(all_data.columns)\ncolumns_to_drop=[]\n# create a list with all columns containing 'lag'\nfor element in columnliste:\n    if 'avg_price' in element:\n        columns_to_drop.append(element)\n    if 'delta_price_lag' in element:\n        columns_to_drop.append(element)\n\nall_data = all_data.drop(columns=columns_to_drop)\n\ntime.time() - ts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del df_sales\nall_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Item Name Features\nI am not satisfied with the name features yet but I am running out of time. Pure code from Gordon Henderson receives better results, I dont understand yet why, as some lines seem strange. Didn't wanna copy his code completely, my version is close enough.\n\nNot all of the item name features are used in the final model. Some made the score worse."},{"metadata":{"trusted":true},"cell_type":"code","source":"# code inspired by Gordon Henderson, adapted: it moves everything in [] to name_sqbarc and everying in () to name_brac. The rest of the item name is in name_cut\n\n# split item names by brackets\ndf_items[\"name_cut\"], df_items[\"name_sqbrac\"] = df_items.item_name.str.split(\"[\", 1).str\ndf_items[\"name_cut\"], df_items[\"name_brac\"] = df_items.name_cut.str.split(\"(\", 1).str\n\n# replace special characters and turn to lower case\ndf_items[\"name_sqbrac\"] = df_items.name_sqbrac.str.replace('[^A-Za-z0-9А-Яа-я]+', \" \").str.lower()\ndf_items[\"name_brac\"] = df_items.name_brac.str.replace('[^A-Za-z0-9А-Яа-я]+', \" \").str.lower()\ndf_items[\"name_cut\"] = df_items.name_cut.str.replace('[^A-Za-z0-9А-Яа-я]+', \" \").str.lower()\n\ndf_items= df_items.fillna('0')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_items.iloc[[2,27,191,1358,3872,3864,4577,6389,7297, 7839 ,9196,10365,22083], :]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_items[\"type\"] = df_items.name_sqbrac.apply(lambda x: x[0:8] if x.split(\" \")[0] == \"xbox\" else x.split(\" \")[0] )\ndf_items.loc[(df_items.type == \"x360\") | (df_items.type == \"xbox360\") | (df_items.type == \"xbox 360\") ,\"type\"] = \"xbox 360\"\n\ndf_items.type = df_items.type.apply( lambda x: x.replace(\" \", \"\") )\ndf_items.loc[ (df_items.type == 'pс' )| (df_items.type == 'pc') | (df_items.type == \"pc\"), \"type\" ] = \"pc\" # for reasons I don't understand pc seems to have different character encodings\ndf_items.loc[ df_items.type == 'рs3' , \"type\"] = \"ps3\"\ndf_items.loc[ df_items.type == \"\", \"type\"] = \"mac\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get first word and first+second word from the rest of the item name\ndf_temp_name = df_items['name_cut'].str.split(n=2, expand=True)\ndf_temp_name.columns = ['word1', 'word2', 'rest']\ndf_temp_name= df_temp_name.fillna(0)\n\ndef set_word12 (row):\n    if row.word2 == 0:\n        return row.word1\n    else:\n        return row.word1 + '-' + row.word2\n    \ndf_temp_name[['word12']] = df_temp_name.apply(lambda row: set_word12(row), axis=1)\n\n# merge info about words back into df_items\ndf_items = pd.concat([df_items, df_temp_name[['word1','word12']]], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check number of unique values in the columns\nprint(df_items.name_cut.nunique())\nprint(df_items.name_sqbrac.nunique())\nprint(df_items.name_brac.nunique())\nprint(df_items.word1.nunique())\nprint(df_items.word12.nunique())\nprint(df_items.type.nunique())\n#df_items.name_sqbrac.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# count how many entries in each of the columns are present less than 40 times\nliste = ('name_cut','name_sqbrac', 'name_brac', 'word1','word12', 'type')\nfor col in liste:\n    group = df_items.groupby([col]).agg({\"item_id\": \"count\"}) # count how many items there are \n    group = group.reset_index()\n    drop_cols = []\n    for cat in group[col].unique():\n        if group.loc[(group[col] == cat), \"item_id\"].values[0] <40:\n            drop_cols.append(cat)\n        \n    print(col,\": no of drop_cols: \", len(drop_cols)   )     ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_items.iloc[[2,27,191,1358,3872,3864,4577,6389,7297, 7839 ,9196,10365,22083], :]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Label Encoding\ndf_items['name_sqbrac'] = le.fit_transform(df_items.name_sqbrac.astype(str))\ndf_items['name_brac'] = le.fit_transform(df_items.name_brac.astype(str))\ndf_items['word1'] = le.fit_transform(df_items.word1.astype(str))\ndf_items['word12'] = le.fit_transform(df_items.word12.astype(str))\ndf_items['type'] = le.fit_transform(df_items.type.astype(str))\n\n# memory saving\ndf_items['name_sqbrac'] = df_items['name_sqbrac'].astype(np.int16)\ndf_items['name_brac'] = df_items['name_brac'].astype(np.int16)\ndf_items['word1'] = df_items['word1'].astype(np.int16)\ndf_items['word12'] = df_items['word12'].astype(np.int16)\ndf_items['type'] = df_items['type'].astype(np.int16)\n\n# merge back into all data\nall_data = pd.merge(all_data, df_items[['item_id','name_sqbrac', 'name_brac', 'word1', 'word12','type']], on= 'item_id', how = 'left')\nall_data.head()\n\ndel df_items","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Sanity Checks & Clipping\nLet's do some sanity checks before saving the data.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data[all_data.date_block_num == 34]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data.isna().any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data.target = all_data.target.clip(0,20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data.target.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# save all_data to be used in next notebook\nall_data.to_pickle('all_data_withFeatures.pkl')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# unused code snippets"},{"metadata":{},"cell_type":"markdown","source":"generating item type\n\ndf_items[\"type\"] = df_items.name_sqbrac.apply(lambda x: x[0:8] if x.split(\" \")[0] == \"xbox\" else x.split(\" \")[0] )\ndf_items.loc[(df_items.type == \"x360\") | (df_items.type == \"xbox360\") | (df_items.type == \"xbox 360\") ,\"type\"] = \"xbox 360\"\ndf_items.loc[ df_items.type == \"\", \"type\"] = \"mac\"\ndf_items.type = df_items.type.apply( lambda x: x.replace(\" \", \"\") )\ndf_items.loc[ (df_items.type == 'pс' )| (df_items.type == 'рс') | (df_items.type == \"pc\"), \"type\" ] = \"pc\" # for reasons I don't understand pc seems to have different character encodings\ndf_items.loc[ df_items.type == 'рs3' , \"type\"] = \"ps3\"\n\ngroup = df_items.groupby([\"type\"]).agg({\"item_id\": \"count\"}) # count how many items there are for each type\ngroup = group.reset_index()\ndrop_cols = []\nfor cat in group.type.unique():\n    if group.loc[(group.type == cat), \"item_id\"].values[0] <40:\n        drop_cols.append(cat)\ndf_items.type = df_items.type.apply( lambda x: \"other\" if (x in drop_cols) else x ) # is applied to name2 in original but why? \ndf_items = df_items.drop([\"type\"], axis = 1)"},{"metadata":{},"cell_type":"markdown","source":"Are there cities that sell more items than other cities? Divide total sales by number of shops in this city. Otherwise Moskow is too high in the sky.\nextract the total number of items sold by one shop over the whole time period\ntotal_items_sold = all_data.groupby('shop_id').agg({'target': 'sum'})\ntotal_items_sold.columns = ['total_items_sold']\nmerge this info into the shops table\ndf_shops = pd.merge(df_shops, total_items_sold, on= 'shop_id', how='left')\nextract how many items have been sold per city. Use 'mean' because this gives the total number divided by the number of shops\ncity_sales_mean = df_shops.groupby('city_code').agg({'total_items_sold': 'mean'})\ncity_sales_mean.columns = ['mean_items_sold']\ncity_sales_total = df_shops.groupby('city_code').agg({'total_items_sold': 'sum'})\ncity_sales_total.columns = ['total_items_sold']\nget number of shops per city into the city_sales df - not needed at the moment\n#no_shops_per_city = df_shops.city_code.value_counts()\n#no_shops_per_city = no_shops_per_city.to_frame().reset_index()\n#no_shops_per_city = no_shops_per_city.rename(columns={'city_code' : 'no_shops_per_city', 'index' : 'city_code'}) \n#city_sales= pd.merge(city_sales, no_shops_per_city, on='city_code', how='left')\nadd average number of items sold per city = total number / number of shops in this city --- this is if you want to calculate 'mean' manually with a lot of code\n#city_sales['avg_items_sold_city'] = city_sales.total_items_sold / city_sales.no_shops_per_city"},{"metadata":{},"cell_type":"markdown","source":"visualize\nfig = plt.figure() # create figure\nax0 = fig.add_subplot(1, 3, 1) # add subplot 1 (1 row, 2 columns, first plot)\nax1 = fig.add_subplot(1, 3, 2) \nax2 = fig.add_subplot(1, 3, 3)\ndf_shops[['total_items_sold']].plot(figsize=(20, 3), ax=ax0)\ncity_sales_total.plot(figsize=(20, 3), ax=ax1)\ncity_sales_mean.plot(figsize=(20, 3), ax=ax2)\nax0.set_title('Total Number of Items Sold per shop_id')\nax0.set_xlabel('shop_id')\nax1.set_title('Total Number of Items Sold per city')\nax1.set_xlabel('city_code')\nax2.set_title('Mean Number of Items Sold per city') # =total city sales / no shops in this city\nax2.set_xlabel('city_code')\nplt.show()"},{"metadata":{},"cell_type":"markdown","source":"df_shops = pd.merge(df_shops, city_sales_mean, on='city_code', how='left')\ndf_shops = df_shops.rename(columns={'mean_items_sold':'mean_items_sold_city', 'total_items_sold':'total_items_sold_shop'})"},{"metadata":{},"cell_type":"markdown","source":"create two new columns ATTENTION: the numbers to devide need to be revisited because of changed clipping strategy, not done yet, I will not use in model\nrepresenting if a shop belongs to a high/medium/low selling city (by mean items sold)\nand if this is a high, medium or low selling shop\n\ndf_shops.loc[df_shops.mean_items_sold_city < 60000, 'mean_items_sold_city'] = 0 # low selling city\ndf_shops.loc[(df_shops.mean_items_sold_city >= 60000) & (df_shops.mean_items_sold_city < 100000), 'mean_items_sold_city'] = 1 # medium selling city\ndf_shops.loc[df_shops.mean_items_sold_city >=100000, 'mean_items_sold_city'] = 2 # high selling city\ndf_shops = df_shops.rename(columns={'mean_items_sold_city':'city_sales_code'})\n\ndf_shops.loc[df_shops.total_items_sold_shop < 80000, 'total_items_sold_shop'] = 0 # low selling shop\ndf_shops.loc[(df_shops.total_items_sold_shop >= 80000) & (df_shops.total_items_sold_shop < 150000), 'total_items_sold_shop'] = 1 # medium selling shop\ndf_shops.loc[df_shops.total_items_sold_shop >=150000, 'total_items_sold_shop'] = 2 # high selling shop\ndf_shops = df_shops.rename(columns={'total_items_sold_shop':'shop_sales_code'})\n\n#df_shops.head()  "},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Code for getting average category price and put either this or average price in the average price column. \n\n#function for checking if there is an average price and returning either it or the average category price\n\n\ndef set_price (row):\n    if np.isnan(row.avg_price):\n        row.avg_price = row.avg_category_price\n    return row.avg_price\n\n#there are 363 new items for which no average price can be calculated, put the average price of the category instead\n\ngroup = all_data.groupby(['item_category_id']).agg({'avg_price':'mean'})\ngroup.columns = ['avg_category_price']\nall_data = pd.merge(all_data, group, on= 'item_category_id', how = 'left')\n\ncol = all_data[all_data.date_block_num == 34].apply (lambda row: set_price(row), axis=1)\n\n#assign values from col to month 34 from all_data\nall_data[all_data.date_block_num == 34] = all_data[all_data.date_block_num == 34].assign(avg_price = col.values) \nall_data.drop(columns='avg_category_price', inplace=True) # no longer needed\n\n"},{"metadata":{},"cell_type":"markdown","source":"# former execution log"},{"metadata":{},"cell_type":"markdown","source":"Version 6:\n\nFeatures used: columns = [\"shop_id\", \"item_id\", \"date_block_num\", \"item_category_id\", 'target_lag_1', 'target_lag_12']\n\nXGBoost : \n\nStopping. Best iteration:\n[70]\tvalidation_0-rmse:2.71430\tvalidation_1-rmse:4.84891\n\n\nTraining time: 381\n\nPublic score: 1.13677"},{"metadata":{},"cell_type":"markdown","source":"Version 9:\n\nFeatures used: columns = ['shop_id', 'item_id', 'date_block_num', 'item_category_id',\n       'month_item_sum', 'month_shop_sum', \n       'target_lag_1', 'target_lag_12',\n       'month_item_sum_lag_1', 'month_item_sum_lag_12', \n       'month_shop_sum_lag_1', 'month_shop_sum_lag_12']\n\nXGBoost : \n\nStopping. Best iteration:\n[128]\tvalidation_0-rmse:2.11774\tvalidation_1-rmse:4.48538\n\n\n\nTraining time: 932\n\nPublic score: 1.18514"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Version 11:\n\nFeatures used: columns = ['shop_id', 'item_id', 'date_block_num', 'item_category_id', **'month_item_sum', 'month_shop_sum',** 'target_lag_1', 'target_lag_12', 'month_item_sum_lag_1', 'month_item_sum_lag_12', 'month_shop_sum_lag_1', 'month_shop_sum_lag_12']\n\n**used cleaned data**. i.e. shop names cleaned AND data clipped after building grid\n\nXGBoost :\n    \nStopping. Best iteration:\n[150]\tvalidation_0-rmse:0.64435\tvalidation_1-rmse:0.73362\n\nTraining time: 1141\n\nPublic score: 1.23784"},{"metadata":{},"cell_type":"markdown","source":"Version 13:\n\nFeatures used: columns = ['shop_id', 'item_id', 'date_block_num', 'item_category_id', 'target_lag_1', 'target_lag_12', 'month_item_sum_lag_1', 'month_item_sum_lag_12', 'month_shop_sum_lag_1', 'month_shop_sum_lag_12']\n\n**used cleaned data**. i.e. shop names cleaned AND data clipped after building grid\n\nXGBoost :\n\nStopping. Best iteration:\n[44]\tvalidation_0-rmse:0.84370\tvalidation_1-rmse:0.92533\n\nTraining time: 369\n\nPublic score: 0.95375"},{"metadata":{},"cell_type":"markdown","source":"\n\nVersion 14:\n\nFeatures used: columns = ['shop_id', 'item_id', 'date_block_num', 'item_category_id',\n       'target_lag_1', 'target_lag_12',\n        'month_item_**mean**_lag_1', 'month_item_**mean**_lag_12', \n       'month_shop_**mean**_lag_1', 'month_shop_**mean**_lag_12']\n\nused cleaned data. i.e. shop names cleaned AND data clipped after building grid\n\nXGBoost :\n\nStopping. Best iteration:\n[28]\tvalidation_0-rmse:0.85613\tvalidation_1-rmse:0.93336\n\nTraining time: 279\n\nPublic score: 0.95996"},{"metadata":{},"cell_type":"markdown","source":"Version 19:\n\nFeatures used: columns = ['shop_id', 'item_id', 'date_block_num', 'target', 'item_category_id',\n           'target_lag_1', 'target_lag_12',\n           'month_item_**sum**_lag_1', 'month_item_**sum**_lag_12', \n           'month_shop_**sum**_lag_1','month_shop_**sum**_lag_12', \n           '**city_code', 'shop_sales_code','city_sales_code'**]\n\nused cleaned data. i.e. shop names cleaned AND data clipped after building grid\n\nXGBoost :\n\n[999]\tvalidation_0-rmse:0.00564\tvalidation_1-rmse:0.00656\n\n\n\nTraining time: 6284\n\nPublic score: I did not dare to submit. the min value of the prediction was 19.47. And I found my error. "},{"metadata":{},"cell_type":"markdown","source":"Version 20:\n\nFeatures used: columns = ['shop_id', 'item_id', 'date_block_num', 'item_category_id', 'target_lag_1', 'target_lag_12', 'month_item_sum_lag_1', 'month_item_sum_lag_12', 'month_shop_sum_lag_1','month_shop_sum_lag_12', 'city_code', 'shop_sales_code','city_sales_code']\n\nused cleaned data. i.e. shop names cleaned AND data clipped after building grid\n\nXGBoost :\n\nStopping. Best iteration:\n[45]\tvalidation_0-rmse:0.84357\tvalidation_1-rmse:0.92975\n\nTraining time: 404\n\nPublic score: 0.96130"},{"metadata":{},"cell_type":"markdown","source":"Version 23:\n\nFeatures used: columns = ['shop_id', 'item_id', 'date_block_num', 'item_category_id',\n           'target_lag_1', 'target_lag_12',\n           'month_item_sum_lag_1', 'month_item_sum_lag_12', \n           'month_shop_sum_lag_1','month_shop_sum_lag_12']  --- like V13\n           \nused new cleaned data. i.e. shop names cleaned AND data clipped before modelling\n\nXGBoost :\nStopping. Best iteration:\n[27]\tvalidation_0-rmse:0.85564\tvalidation_1-rmse:0.93935\n\nTraining time: 342\n\nPublic score: 0.96739"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Version 24:\n\nFeatures used: columns = ['shop_id', 'item_id', 'date_block_num', 'item_category_id',\n           'target_lag_1', 'target_lag_12',\n           'month_item_sum_lag_1',**'month_item_sum_lag_2','month_item_sum_lag_3**', 'month_item_sum_lag_12', \n           'month_shop_sum_lag_1',**'month_shop_sum_lag_2','month_shop_sum_lag_3'**,'month_shop_sum_lag_12'] \n           \nused new cleaned data. i.e. shop names cleaned AND data clipped before modelling\n\nXGBoost :\nStopping. Best iteration:\n[26]\tvalidation_0-rmse:0.84397\tvalidation_1-rmse:0.93156\n\nTraining time: 432\n\nPublic score: 0.95726"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Version 27:\n\nFeatures used: columns = ['shop_id', 'item_id', 'date_block_num', 'item_category_id',\n           'city_code', 'item_bcat_code', \n           'target_lag_1', 'target_lag_12',\n           'month_item_sum_lag_1','month_item_sum_lag_2','month_item_sum_lag_3', 'month_item_sum_lag_12', \n           'month_shop_sum_lag_1','month_shop_sum_lag_2','month_shop_sum_lag_3','month_shop_sum_lag_12',] \n           \nused new cleaned data. i.e. shop names cleaned AND data clipped before modelling\n\nXGBoost :\nStopping. Best iteration:\n[35]\tvalidation_0-rmse:0.83877\tvalidation_1-rmse:0.92985\n\nTraining time: 394\n\nPublic score: 0.95394"},{"metadata":{},"cell_type":"markdown","source":"Version 28:\n\nFeatures used: columns = ['shop_id', 'item_id', 'date_block_num', 'item_category_id',\n           'city_code', 'item_bcat_code', \n           'target_lag_1', 'target_lag_12',\n           'month_item_sum_lag_1','month_item_sum_lag_2','month_item_sum_lag_3', 'month_item_sum_lag_12', \n           'month_shop_sum_lag_1','month_shop_sum_lag_2','month_shop_sum_lag_3','month_shop_sum_lag_12',] \n           \nused new cleaned data. i.e. shop names cleaned AND data clipped before modelling\n\nXGBoost : **changed eta to 0.2 (from 0.3)**\n\nStopping. Best iteration:\n[60]\tvalidation_0-rmse:0.83157\tvalidation_1-rmse:0.92357\n\nTraining time: 590\n\nPublic score: 0,94763"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Version 29 (genau wie v31, beim Speichern ging was schief):\n\nFeatures used: columns = ['shop_id', 'item_id', 'date_block_num', 'item_category_id',\n       'city_code', 'item_bcat_code', \n       'days_per_month', 'month',\n       'target_lag_1', 'target_lag_2', 'target_lag_3', 'target_lag_12',\n       'month_item_sum_lag_1', 'month_item_sum_lag_2', 'month_item_sum_lag_3',\n       'month_item_sum_lag_12', 'month_shop_sum_lag_1', 'month_shop_sum_lag_2',\n       'month_shop_sum_lag_3', 'month_shop_sum_lag_12', 'month_sum_lag_1',\n       'month_sum_lag_2', 'month_sum_lag_3', 'month_sum_lag_12',\n       'monthly_transactions_lag_1', 'monthly_transactions_lag_2',\n       'monthly_transactions_lag_3', 'monthly_transactions_lag_12']\n           \nused new cleaned data. i.e. shop names cleaned AND data clipped before modelling\n\nXGBoost : **changed eta back to 0.3, because of faster training**\n\nStopping. Best iteration:\n[5]\tvalidation_0-rmse:0.87967\tvalidation_1-rmse:0.94554\n\n\nTraining time: 239 (191 in v31)\n\nPublic score: 0,96909"},{"metadata":{},"cell_type":"markdown","source":"Version 35:\n\nFeatures used: = ['shop_id', 'item_id', 'date_block_num', 'item_category_id',\n       'city_code', 'item_bcat_code', \n       'days_per_month', 'month', '**avg_price**',\n       'target_lag_1', 'target_lag_2', 'target_lag_3', 'target_lag_12',\n       'month_item_sum_lag_1', 'month_item_sum_lag_2', 'month_item_sum_lag_3',\n       'month_item_sum_lag_12', 'month_shop_sum_lag_1', 'month_shop_sum_lag_2',\n       'month_shop_sum_lag_3', 'month_shop_sum_lag_12', 'month_sum_lag_1',\n       'month_sum_lag_2', 'month_sum_lag_3', 'month_sum_lag_12',\n       'monthly_transactions_lag_1', 'monthly_transactions_lag_2',\n       'monthly_transactions_lag_3', 'monthly_transactions_lag_12']\n           \nused new cleaned data. i.e. shop names cleaned AND data clipped before modelling\n\nXGBoost : \n\nStopping. Best iteration:\n[16]\tvalidation_0-rmse:0.83772\tvalidation_1-rmse:0.94485\n\n\nTraining time: 321\n\nPublic score: 0,95793"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Version 36:\n\nFeatures used: columns = ['shop_id', 'item_id', 'date_block_num', 'item_category_id',\n       'city_code', 'item_bcat_code', \n       'days_per_month', 'month', 'avg_price',\n       'target_lag_1', 'target_lag_2', 'target_lag_3',\n       'month_item_sum_lag_1', 'month_item_sum_lag_2', 'month_item_sum_lag_3',\n        'month_shop_sum_lag_1', 'month_shop_sum_lag_2','month_shop_sum_lag_3', \n        'month_sum_lag_1','month_sum_lag_2', 'month_sum_lag_3',\n       'monthly_transactions_lag_1', 'monthly_transactions_lag_2','monthly_transactions_lag_3']\n           \nused new cleaned data. i.e. shop names cleaned AND data clipped before modelling, removed 12er lags and more month training data\n\nXGBoost : \n\nStopping. Best iteration:\n[16]\tvalidation_0-rmse:0.86907\tvalidation_1-rmse:0.95285\n\n\nTraining time: 539\n\nPublic score: 0,95022"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}