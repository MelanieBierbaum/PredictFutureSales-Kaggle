{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n\n# load packages\nimport sys # access to system parameters https://docs.python.org/3/library/sys.html\nprint(\"Python version: {}\". format(sys.version))\n\nimport numpy as np # scientific computing\nprint(\"NumPy version: {}\". format(np.__version__))\n\nimport pandas as pd # data processing and analysis \nprint(\"pandas version: {}\". format(pd.__version__))\n\nimport matplotlib\nimport matplotlib.pyplot as plt # plotting\n%matplotlib inline \nprint(\"matplotlib version: {}\". format(matplotlib.__version__))\n\nimport seaborn as sns\nprint(\"seaborn version: {}\". format(sns.__version__))\n \nimport sklearn # machine learning algorithms\nprint(\"scikit-learn version: {}\". format(sklearn.__version__))\n\nfrom itertools import product\n\nimport time\nimport pickle\n\nprint(\"----------\")\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# read csv file into data frames\ndf_items = pd.read_csv('../input/competitive-data-science-predict-future-sales/items.csv')\ndf_shops = pd.read_csv('../input/competitive-data-science-predict-future-sales/shops.csv')\ndf_sales_raw = pd.read_csv('../input/competitive-data-science-predict-future-sales/sales_train.csv')\ndf_sales = df_sales_raw.copy() # make a copy of the raw data\ndf_test = pd.read_csv('../input/competitive-data-science-predict-future-sales/test.csv')\ndf_item_cat = pd.read_csv('../input/competitive-data-science-predict-future-sales/item_categories.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sales.tail(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Cleanup"},{"metadata":{},"cell_type":"markdown","source":"## Shops\nSome shops are duplicates according to their shop name and the time when they have sales. Luckily, only one of the duplicated shop_ids is in the test set. The shop_id from the test set will remain.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert data to datetime object\ndf_sales['date'] = pd.to_datetime(df_sales['date'], dayfirst=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check minimum and maximum sales date of the shops with similar names\nprint(\"Sales period shop_id =1: \", df_sales[df_sales.shop_id==1]['date'].min(), df_sales[df_sales.shop_id==1]['date'].max())\nprint(\"Sales period shop_id =58: \", df_sales[df_sales.shop_id==58]['date'].min(), df_sales[df_sales.shop_id==58]['date'].max())\nprint(\"-----\")\nprint(\"Sales period shop_id =0: \", df_sales[df_sales.shop_id==0]['date'].min(), df_sales[df_sales.shop_id==0]['date'].max())\nprint(\"Sales period shop_id =57: \", df_sales[df_sales.shop_id==57]['date'].min(), df_sales[df_sales.shop_id==57]['date'].max())\nprint(\"-----\")\nprint(\"Sales period shop_id =10: \", df_sales[df_sales.shop_id==10]['date'].min(), df_sales[df_sales.shop_id==10]['date'].max())\nprint(\"Sales period shop_id =11: \", df_sales[df_sales.shop_id==11]['date'].min(), df_sales[df_sales.shop_id==11]['date'].max())\nprint(\"----- 10 has a sales hole that is filled by 11\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# make duplicated shops one shop, use the shop_id present in the test set (df_test[df_test.shop_id==0].shape[0] -> 0)\n# in df_sales\n# Жуковский ул. Чкалова 39м² -> Жуковский ул. Чкалова 39м?\ndf_sales.loc[df_sales.shop_id == 11, 'shop_id'] = 10\n# !Якутск ТЦ \"Центральный\" фран -> Якутск ТЦ \"Центральный\"\ndf_sales.loc[df_sales.shop_id == 1, 'shop_id'] = 58\n# !Якутск Орджоникидзе, 56 фран -> Якутск Орджоникидзе, 56\ndf_sales.loc[df_sales.shop_id == 0, 'shop_id'] = 57\n# remove from df_shops as well\ndf_shops.drop([0,1,11], axis=0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Items"},{"metadata":{"trusted":true},"cell_type":"code","source":"# boxplot of item_price: the vast majority of item_price is below 40000. One outlier with over 300000.\nplt.figure(figsize=(20,3))\nsns.boxplot(x=['item_price'], data=df_sales)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# item_price\nprint(\"Minimum item price: \", df_sales.item_price.min())\nprint(\"Maximum item price: \", df_sales.item_price.max())\nitem_price_max = df_sales.loc[df_sales.item_price == df_sales.item_price.max()]\n#df_items[df_items.item_id==item_id_maximum_price]\nprint(\"\\nRows with maximum item price:\\n \",item_price_max)\nprint(\"\\nItem with highest price:\\n \", df_items[df_items.item_id==6066]) # dirty, dirty, dirty, improve later\n# this is a software for remotecontrolling PCs called Radmin3, it was sold for 522 persons, high price could be plausible\nitem_price_min = df_sales[df_sales.item_price == df_sales.item_price.min()]\nprint(\"\\nRows with minimum item price:\\n \",item_price_min)\n# there is just one row with an item_price below 0, remove it\ndf_sales = df_sales[df_sales.item_price > 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's check sales of price outlier - it has been sold only once and is not included in the test set. remove it\n#df_sales[df_sales.item_id == 6066]\n#df_test[df_test.item_id == 6066]\ndf_sales = df_sales[df_sales.item_price < 100000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# boxplot of item_cnt_day: the vast majority of item_cnt_day is around 0-100. A few negative values show up (returns) and 2 outliers (approx 1000/2200)\nplt.figure(figsize=(20,3))\nsns.boxplot(x=['item_cnt_day'], data=df_sales)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Minimum item_cnt_day: \", df_sales.item_cnt_day.min())\nprint(\"Maximum item_cnt_day: \", df_sales.item_cnt_day.max())\n\nitem_cnt_day_max = df_sales[df_sales.item_cnt_day == df_sales.item_cnt_day.max()]\nprint(\"\\nRows with maximum daily sales amount:\\n \",item_cnt_day_max)\nprint(\"\\nItem sold most often:\\n \", df_items[df_items.item_id==11373]) # dirty, dirty, dirty, improve later\n# looks like this is shipping cost. It is only present for shop_id 12 which seems to be the online shop\n\nitem_cnt_day_b0 = df_sales[df_sales.item_cnt_day < 0]\nprint(\"\\nNumber of Rows with sales below 0, i.e. returns: \",item_cnt_day_b0.shape[0])\nprint(\"Percentage of Rows with sales below 0, i.e. returns: % 1.2f \" %(item_cnt_day_b0.shape[0] /df_sales.shape[0]*100) )\n# the percentage of rows with returns is low, but before discarding them, let's look again after monthly aggregation of data. ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get the second highest sold item\n#df_sales.item_cnt_day.sort_values(ascending=False)[0:3]\n#df_sales.loc[2326930]\n#df_items[df_items.item_id == 20949]\n# 2nd highest sold item, Фирменный пакет майка 1С Интерес белый (34*42).., plastic bag?","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's quickly look at how the sales of the 2 highest sold items looks over time: item_id 11373 and 20949\nitem11373 = df_sales[df_sales.item_id == 11373]\nitem20949 = df_sales[df_sales.item_id == 20949]\nfig = plt.figure() # create figure\nax0 = fig.add_subplot(1, 2, 1) # add subplot 1 (1 row, 2 columns, first plot)\nax1 = fig.add_subplot(1, 2, 2) \nitem11373.plot(x='date',y='item_cnt_day', figsize=(20,4), ax=ax0)\nitem20949.plot(x='date',y='item_cnt_day', figsize=(20,4), ax=ax1)\nax0.set_title(\"Item 11373: no of items sold over time\")\nax1.set_title(\"Item 20949: no of items sold over time\")\nplt.show()\n# it can be seen that the high sales is only once. I decide to drop this row.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# everything with sales 1000 and bigger is declared as outlier based on the investigations before and removed\ndf_sales = df_sales[df_sales.item_cnt_day < 1000]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Aggregation to monthly values\n\nLike shown in Programming Assignment week3. The column with monthly aggregation is called 'target'. Could have also been called 'item_cnt_month'."},{"metadata":{"trusted":true},"cell_type":"code","source":"index_cols = ['shop_id', 'item_id', 'date_block_num']\n\n# For every month we create a grid from all shops/items combinations from that month\ngrid = [] \nfor block_num in df_sales['date_block_num'].unique():\n    cur_shops = df_sales[df_sales['date_block_num']==block_num]['shop_id'].unique()\n    cur_items = df_sales[df_sales['date_block_num']==block_num]['item_id'].unique()\n    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))\n\n#turn the grid into pandas dataframe\ngrid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)\n\n#get aggregated values for (shop_id, item_id, month)\n#gb = df_sales.groupby(index_cols,as_index=False).agg({'item_cnt_day':'sum','item_price':'mean'})\ngb = df_sales.groupby(index_cols,as_index=False).agg({'item_cnt_day':'sum'})\n#gb = gb.rename(columns={'item_cnt_day':'target', 'item_price':'mean_item_price'})\ngb = gb.rename(columns={'item_cnt_day':'target'})\n\n#join aggregated data to the grid\nall_data = pd.merge(grid,gb,how='left',on=index_cols).fillna(0)\n\n# this grid has lots of 0 target values, approx 85%. \n# all shop_ids are combined with all item_ids from one month. If an item has been sold in a month but from another shop, all shops\n# where the item has not been sold will get a target of 0.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# downcast data type for better memory usage\nall_data['shop_id'] = all_data['shop_id'].astype(np.int8)\nall_data['item_id'] = all_data['item_id'].astype(np.int16)\nall_data['date_block_num'] = all_data['date_block_num'].astype(np.int8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's look again at the monthly items sold."},{"metadata":{"trusted":true},"cell_type":"code","source":"# boxplot of monthly item sold sum: the biggest outlier is still there. It must have only been sold once.\nplt.figure(figsize=(20,3))\nsns.boxplot(x=['target'], data=all_data)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data[all_data.target < 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data[all_data.target > 1500]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After aggregation there are still negative item counts.\nBUT: in the final submission the target values have to be clipped to be in 0-20 range. I wonder if the right place for clipping is before modelling or after. Gordon Henderson suggests in the forum to do it after feature engineering, just before modelling. dlarionov clips once just after building the grid, and again before submission.\n\nI decidied to clip before modelling now. And again before submission. keep the higher numbers for feature engineering."},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data.target.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preprocessing\n## Monthly info"},{"metadata":{"trusted":true},"cell_type":"code","source":"# create data frame to store all monthly values\n# first get the number of transactions that have been performed in each month\nmonth_info = df_sales.date_block_num.value_counts()\nmonth_info = month_info.to_frame(name='monthly_transactions').reset_index()\nmonth_info = month_info.rename(columns={'index':'date_block_num'})\nmonth_info = month_info.sort_values(by='date_block_num', ascending=True)\n\n# then get the total items sold each month\n#month_sum_clipped = all_data.groupby('date_block_num')['target'].sum()\nmonth_sum = df_sales.groupby('date_block_num')['item_cnt_day'].sum()\nmonth_info=pd.merge(month_info,month_sum.to_frame(), on='date_block_num')\n#month_info=pd.merge(month_info,month_sum_clipped.to_frame(), on='date_block_num')\nmonth_info = month_info.rename(columns={'item_cnt_day':'month_sum'})\n#month_info = month_info.rename(columns={'target':'month_clip_sum'})\n\n# add the number of days per month\ndays_per_month = [31,28,31,30,31,30,31,31,30,31,30,31,\n                  31,28,31,30,31,30,31,31,30,31,30,31,\n                 31,28,31,30,31,30,31,31,30,31]\nmonth_info['days_per_month'] = days_per_month\n\n# add number of month: 1= january and so on\nmonth = [1,2,3,4,5,6,7,8,9,10,11,12,\n        1,2,3,4,5,6,7,8,9,10,11,12,\n        1,2,3,4,5,6,7,8,9,10]\nmonth_info['month'] = month\n\nmonth_info.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# month_info.plot(x='date_block_num', y=['monthly_transactions','month_sum', 'month_clip_sum'], figsize=(10,6))\nmonth_info.plot(x='date_block_num', y=['monthly_transactions','month_sum'], figsize=(10,6))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# downcast data type for better memory usage\nmonth_info['date_block_num'] = month_info['date_block_num'].astype(np.int8)\nmonth_info['monthly_transactions'] = month_info['monthly_transactions'].astype(np.int32)\nmonth_info['days_per_month'] = month_info['days_per_month'].astype(np.int8)\nmonth_info['month'] = month_info['month'].astype(np.int8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Shop Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# extract city name\ndf_shops['city']= df_shops.shop_name.str.split(' ', expand=True)[0]\n# change city to 'other' for shop_ids 55, 12 and 9. Their translated names are Digital Warehouse 1C-Online, Online shop Emergency and Outbound Trade.\ndf_shops.loc[df_shops.shop_id == 55, 'city'] = 'other'\ndf_shops.loc[df_shops.shop_id == 12, 'city'] = 'other'\ndf_shops.loc[df_shops.shop_id == 9, 'city'] = 'other'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# downcast data type for better memory usage\ndf_shops['shop_id'] = df_shops['shop_id'].astype(np.int8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Item Category Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_item_cat['item_broad_cat']= df_item_cat.item_category_name.str.split(' - ', expand=True)[0]\n# correct sub category names and set those with fewer thatn 3 occurances to 'other'\ndf_item_cat.loc[26:31, 'item_broad_cat'] = 'Игры'\ndf_item_cat.loc[32, 'item_broad_cat'] = 'Карты оплаты'\ndf_item_cat.loc[0, 'item_broad_cat'] = 'other'\ndf_item_cat.loc[8:9, 'item_broad_cat'] = 'other'\ndf_item_cat.loc[79:83, 'item_broad_cat'] = 'other'\ndf_item_cat.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#pd.set_option('display.max_rows', 500)\ndf_item_cat[\"split\"] = df_item_cat.item_category_name.apply(lambda x: x.split(\"-\"))\ndf_item_cat[\"item_sub_cat\"] = df_item_cat.split.apply(lambda x: x[1].strip() if len(x) > 1 else x[0].strip())\ndf_item_cat = df_item_cat.drop(columns='split')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Generate Output"},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data.to_pickle('all_data_cleaned.pkl') # use pickle because reading/writing is faster than to csv\ndf_shops.to_csv('df_shops_cleaned.csv')\nmonth_info.to_pickle('month_info.pkl')\ndf_item_cat.to_csv('df_item_cat_cleaned.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# all_data = pd.read_pickle('all_data_cleaned.pkl')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"..\n\n..\n\n..\n\n..\n..\n..\n..\n..\n..\n..\n..\nData Preprocessing based on item_name - discontinued, too much effort for uncertain return"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_item_test = pd.merge(df_items, df_item_cat, on='item_category_id', how='left')\ndf_item_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# not continued here, seen as too much effort for uncertain return\ndef create_sub_cat (row):\n    if '1С:Аудиокниги' in row['item_name']:\n        return 'Audiobooks'\n    if '1С:Аудио'  in row['item_name']:\n        return 'Audio'\n    if '1С:Аудиотеатр' in row['item_name']:\n        return 'Audiotheater'\n    if '1С:Образовательная' in row['item_name']:\n        return 'Educational'\n    if '1С:Познавательная' in row['item_name']:\n        return 'Cognitive'\n    if '1С:Предпр' in row['item_name']:\n        return 'Enterprise'\n    if '1С:Репетитор' in row['item_name']:\n        return 'Tutor'\n    if '1С:Предпр' in row['item_name']:\n        return 'Enterprise'\n    if '1С:Управл' in row['item_name']:\n        return 'Management'\n    if '1С:Школа' in row['item_name']:\n        return 'School'\n    if 'обучение' in row['item_name']:\n        return 'School'\n    if '3D Action Puzzle' in row['item_name']:\n        return '3D Action Puzzle'\n    if '3D Crystal Puzzle' in row['item_name']:\n        return '3D Crystal Puzzle'\n    if '3D Puzzle' in row['item_name']:\n        return '3D Puzzle'\n    if '3D Сфера-Пазлы' in row['item_name']:\n        return '3D Sphere Puzzle'\n    if '4D пазлы' in row['item_name']:\n        return '4D Puzzle'\n    if '3D Сфера-Пазлы' in row['item_name']:\n        return '3D Sphere Puzzle'\n    if '3D Сфера-Пазлы' in row['item_name']:\n        return '3D Sphere Puzzle'\n    if '3D Сфера-Пазлы' in row['item_name']:\n        return '3D Sphere Puzzle'\n    if '3D Сфера-Пазлы' in row['item_name']:\n        return '3D Sphere Puzzle'\n    return 'Other'\n   \n# how to use\n#col = df_item_test.apply (lambda row: create_sub_cat(row), axis=1)\n#df_item_test = df_item_test.assign(sub_cat=col.values) # assign values to column 'sub_cat'\n#df_item_test[110:120]","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}